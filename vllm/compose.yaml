version: "3.9"

x-vllm-base: &vllm-base
  command: >-
    --model ${VLLM_MODEL:-NousResearch/Meta-Llama-3-8B-Instruct}
    --port ${VLLM_PORT:-8000} ${VLLM_ARGS:-}
  ports:
    - "${VLLM_PORT:-8000}:${VLLM_PORT:-8000}"
  environment:
    HF_HOME: /data/hf
  volumes:
    - ../data/hf:/data/hf
  healthcheck:
    test: ["CMD-SHELL", "curl -fsS http://localhost:${VLLM_PORT:-8000}/health || exit 1"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 20s

services:
  gateway:
    build: ../api
    ports:
      - "${GATEWAY_PORT:-8080}:8080"
    environment:
      MODELS_CONFIG: /config/models.yaml
    volumes:
      - ../config/models.vllm.yaml:/config/models.yaml:ro
    depends_on:
      vllm-cuda:
        condition: service_healthy
      vllm-rocm:
        condition: service_healthy
    profiles: ["cuda", "rocm"]

  vllm-cuda:
    <<: *vllm-base
    build:
      context: .
      args:
        BASE_IMAGE: ${VLLM_IMAGE_CUDA:-vllm/vllm-openai:v0.10.0}
    profiles: ["cuda"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]

  vllm-rocm:
    <<: *vllm-base
    build:
      context: .
      args:
        BASE_IMAGE: ${VLLM_IMAGE_ROCM:-vllm/vllm-openai-rocm:v0.10.0}
    profiles: ["rocm"]
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
    deploy:
      resources:
        reservations:
          devices:
            - driver: amd
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
